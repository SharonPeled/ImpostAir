project:
  experiment_name: "SCAT_patch_transformer_debug"
  run_name: "run1"
  seed: 123

paths:  # main relative paths
  data_dir: "./sandbox/SCAT_unzipped"
  mlflow_uri: "./mlruns"
  checkpoint_dir: "./models"
  predictions_dir: "./predictions"

compute:
  accelerator: "gpu"
  devices: 1
  num_workers: 4

transformations:
  normalization: 
    type: "standard"
    params:
      mean: [58.0, 15.0, 350.0, 100.0, 100.0]  # Mean for each channel
      std: [1.0, 1.0, 100.0, 50.0, 50.0]   # Std for each channel

data:
  class_path: "src.data.SCATDataset.SCATDataset"
  # Feature configuration
  input_features: ['lat', 'lon', 'alt', 'vx', 'vy']  # TODO: it assumes standardized timestamps
  train_ratio: 0.7
  val_ratio: 0.15

model:
  class_path: "src.models.PatchTransformerForecaster.PatchTransformerForecaster"

  patch_len: 16          # Length of each patch (subseries length)
  stride: 16             # Stride for patch extraction (non-overlapping patches)
  
  d_model: 128           # Model dimension (smaller due to patch efficiency)
  n_heads: 16            # Number of attention heads (PatchTST uses 16)
  n_layers: 3            # Number of transformer layers (fewer needed with patches)
  d_ff: 256              # Feed-forward dimension (2x d_model typical)
  dropout: 0.2           # Dropout rate
  activation: "gelu"     # Activation function (GELU as in PatchTST)
  
  # Input/Output parameters
  num_features: 5        # Number of input features (lat, lon, alt, vx, vy) - not including timestamp
  max_num_patches: 300    # Maximum number of patches for positional encoding
  context_length: 320    # Maximum context length (20 patches * 16 timesteps)
  pos_encoding_type: "learnable"  # "learnable" or "sinusoidal"

training:
  batch_size: 1
  accumulate_grad_batches: 10  # number of batches to accumulate before updating the model
  
  # Training parameters
  max_epochs: 15
  learning_rate: 0.0001     # PatchTST uses 1e-4
  weight_decay: 0.01        # L2 regularization as in PatchTST
  gradient_clip_val: 1.0
  
  # Early stopping
  early_stopping_patience: 20   # Longer patience for patch-based training
  early_stopping_monitor: "val_loss"
  
  # Progressive training parameters
  teacher_forcing_ratio: 1.0    # Always use ground truth during training
  autoregressive_inference: true  # Pure autoregressive during inference

debug:
  # activate: true
  sample_size: # 30
