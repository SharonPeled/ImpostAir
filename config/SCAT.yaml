project:
  experiment_name: "patch_transformer_progressive_scat"
  run_name: "run1"
  seed: 42

paths:  # main relative paths
  data_dir: "./sandbox/SCAT_unzipped"
  mlflow_uri: "./mlruns"
  checkpoint_dir: "./models"
  predictions_dir: "./predictions"

compute:
  accelerator: "gpu"
  devices: 1
  num_workers: 4

transformations:
  normalization: 
    type: "standard"
    params:
      mean: [480000, 300, 75, 1100, 500]  # Mean for each channel
      std: [500, 2.5, 10, 8, 40]   # Std for each channel
  handle_missing: "forward_fill"

data:
  class_path: "src.data.SCATDataset.SCATDataset"
  # Feature configuration
  input_features: ['lat', 'lon', 'alt', 'vx', 'vy']  # assumes standardized timestamps
  train_ratio: 0.7
  val_ratio: 0.15

model:
  class_path: "src.models.PatchTransformerForecaster.PatchTransformerForecaster"
  
  # Patch-based parameters (inspired by PatchTST)
  patch_len: 16          # Length of each patch (subseries length)
  stride: 16             # Stride for patch extraction (non-overlapping patches)
  
  # Architecture parameters (adapted from PatchTST defaults)
  d_model: 128           # Model dimension (smaller due to patch efficiency)
  n_heads: 16            # Number of attention heads (PatchTST uses 16)
  n_layers: 3            # Number of transformer layers (fewer needed with patches)
  d_ff: 256              # Feed-forward dimension (2x d_model typical)
  dropout: 0.2           # Dropout rate
  activation: "gelu"     # Activation function (GELU as in PatchTST)
  
  # Input/Output parameters
  num_features: 5        # Number of input features (lat, lon, alt, vx, vy, timestamp)
  max_num_patches: 300    # Maximum number of patches for positional encoding
  
  # Progressive training parameters
  context_length: 320    # Maximum context length (20 patches * 16 timesteps)
  # Note: Training generates samples with 1 to 20 context patches
  # Sample 1: 1 patch (16 timesteps) -> predict patch 2
  # Sample 2: 2 patches (32 timesteps) -> predict patch 3
  # ...
  # Sample 20: 20 patches (320 timesteps) -> predict patch 21
  
  # Positional encoding
  pos_encoding_type: "learnable"  # "learnable" or "sinusoidal"

training:
  batch_size: 1
  accumulate_grad_batches: 10  # number of batches to accumulate before updating the model
  
  # Training parameters
  max_epochs: 2
  learning_rate: 0.0001     # PatchTST uses 1e-4
  weight_decay: 0.01        # L2 regularization as in PatchTST
  gradient_clip_val: 1.0
  
  # Early stopping
  early_stopping_patience: 20   # Longer patience for patch-based training
  early_stopping_monitor: "val_loss"
  
  # Progressive training parameters
  teacher_forcing_ratio: 1.0    # Always use ground truth during training
  autoregressive_inference: true  # Pure autoregressive during inference

debug:
  sample_size: 30
