import lightning as pl
from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint
from pathlib import Path
from torchvision import transforms
import datetime
from lightning.pytorch.loggers import MLFlowLogger
import torch

from src.data.GeneralTrajectoryDataModule import GeneralTrajectoryDataModule
from src.objects.GlobalStandardNormalizer import GlobalStandardNormalizer
from src.utils import get_class_from_path


def run_training(config):
    """Train a model using the provided config."""
    
    # Set random seed for reproducibility
    pl.seed_everything(config['project']['seed'])
    timestamp = datetime.datetime.now().strftime('%Y_%m_%d_%H%M%S')

    torch.autograd.set_detect_anomaly(True)

    # initialize transforms
    transform_list = []
    if config['transformations']['normalization']['type'] == 'standard':
        mean = config['transformations']['normalization']['params']['mean']
        std = config['transformations']['normalization']['params']['std']
        transform_list.append(GlobalStandardNormalizer(mean=mean, std=std))

    transform = transforms.Compose(transform_list)

    # Initialize data module
    print("Setting up data module...")
    data_module = GeneralTrajectoryDataModule(config=config, transform=transform)
    data_module.setup()
    
    # Initialize model
    print("Initializing model...")
    model_class = get_class_from_path(config['model']['class_path'])
    model = model_class(config=config)
    
    # Setup callbacks
    callbacks = []
    
    # Early stopping
    if config['training'].get('early_stopping_monitor', None) is not None:
        early_stopping = EarlyStopping(
            monitor=config['training']['early_stopping_monitor'],
            patience=config['training']['early_stopping_patience'],
            mode='min',
            verbose=True
        )
        callbacks.append(early_stopping)
    
    # Model checkpointing
    if config['paths'].get('checkpoint_dir', None) is not None:
        checkpoint_dir = Path(config['paths']['checkpoint_dir'])
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        experiment_name = config['project']['experiment_name']
        run_name = config['project']['run_name']
        model_checkpoint = ModelCheckpoint(
            dirpath=checkpoint_dir,
            filename=f"{timestamp}-{experiment_name}-{run_name}-{{epoch:02d}}-{{val_loss:.2f}}",
            monitor=None,  # We're not tracking any metric
            mode='min',
            save_top_k=1,  # Only keep one checkpoint
            every_n_epochs=1,
            save_on_train_epoch_end=True,  # or False depending on when you want to save
            verbose=True
        )
        callbacks.append(model_checkpoint)
    
    # MLflow logger setup
    assert 'mlflow_uri' in config['paths'], "mlflow_uri must be specified in config"
    run_name_base = config['project'].get('run_name', 'run')
    run_name = f"{run_name_base}_{timestamp}"
    mlflow_logger = MLFlowLogger(
        experiment_name=config['project']['experiment_name'],
        run_name=run_name,
        tracking_uri=config['paths']['mlflow_uri']
    )
        
    trainer = pl.Trainer(
        max_epochs=config['training']['max_epochs'],
        accelerator=config['compute']['accelerator'],
        devices=config['compute']['devices'],
        
        # Gradient accumulation configuration
        accumulate_grad_batches=config['training'].get('accumulate_grad_batches', 1),
        
        # Gradient clipping
        gradient_clip_val=config['training'].get('gradient_clip_val'),
        
        # Logging and checkpointing
        logger=mlflow_logger,
        callbacks=callbacks,
        
        # Progress bar and logging
        enable_progress_bar=True,
        # log_every_n_steps=1,
        
        # Validation
        check_val_every_n_epoch=1,
        
        enable_model_summary=True,
        
        # Deterministic training
        deterministic=True
    )
    
    # Train the model
    print("Starting training...")
    trainer.fit(model, data_module)
    
    # Print best checkpoint info
    print(f"\nâœ“ Training completed!")
    print(f"Model checkpoints saved to: {config['paths'].get('checkpoint_dir', None)}")

    # Evaluate on test set
    print("\nEvaluating on test set...")
    test_results = trainer.test(model, datamodule=data_module)
    print(f"Test results: {test_results}")
    
    return model, trainer
